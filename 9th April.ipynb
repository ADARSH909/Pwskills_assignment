{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c179211-4f0d-42e9-aaab-c4de2a4d50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Bayes' theorem?\n",
    "\n",
    "Bayes' theorem is a fundamental principle in probability theory that describes the probability of an event based on prior knowledge of conditions that might be related to the event. It is named after the Reverend Thomas Bayes, who introduced the concept. Bayes' theorem is widely used in statistics, machine learning, and various fields for making predictions or inferences based on available evidence.\n",
    "\n",
    "Q2. What is the formula for Bayes' theorem?\n",
    "\n",
    "The formula for Bayes' theorem is expressed as:\n",
    "\n",
    "P(A∣B)=P(B∣A)⋅P(A)P(B)P(A∣B)=P(B)P(B∣A)⋅P(A)​\n",
    "\n",
    "where:\n",
    "\n",
    "    P(A∣B)P(A∣B) is the probability of event A occurring given that event B has occurred (the posterior probability).\n",
    "    P(B∣A)P(B∣A) is the probability of event B occurring given that event A has occurred (the likelihood).\n",
    "    P(A)P(A) is the probability of event A occurring (the prior probability).\n",
    "    P(B)P(B) is the probability of event B occurring.\n",
    "\n",
    "Q3. How is Bayes' theorem used in practice?\n",
    "\n",
    "Bayes' theorem is used in various fields for making predictions or updating probabilities based on new evidence. Common applications include:\n",
    "\n",
    "    Spam Filtering: Determining the probability that an email is spam given certain words or features.\n",
    "    Medical Diagnosis: Assessing the probability of a disease given certain symptoms or test results.\n",
    "    Document Classification: Categorizing documents based on the occurrence of certain words.\n",
    "\n",
    "Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "\n",
    "Bayes' theorem is derived from conditional probability. Conditional probability is the probability of an event occurring given that another event has occurred. Bayes' theorem provides a way to update or revise the probability of an event based on new evidence or information. The relationship is evident in the formula, where P(A∣B)P(A∣B) is the conditional probability of A given B.\n",
    "\n",
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "\n",
    "There are three main types of Naive Bayes classifiers:\n",
    "\n",
    "    Gaussian Naive Bayes: Assumes that continuous features follow a Gaussian distribution.\n",
    "    Multinomial Naive Bayes: Suitable for discrete data like text, where features represent the frequency of words.\n",
    "    Bernoulli Naive Bayes: Designed for binary or boolean features, often used in text classification problems.\n",
    "\n",
    "The choice depends on the nature of the data:\n",
    "\n",
    "    Use Gaussian Naive Bayes when dealing with continuous numerical features.\n",
    "    Use Multinomial Naive Bayes for discrete features, especially in text classification problems.\n",
    "    Use Bernoulli Naive Bayes for binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af46fa92-a03a-4f38-81f6-b73bcbe5c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Naive Bayes classification, we use Bayes' theorem to calculate the probability of each class given the observed features. The class with the highest probability is predicted. Here's how you can calculate it based on the provided dataset:\n",
    "\n",
    "Given:\n",
    "\n",
    "    Features X1=3X1​=3 and X2=4X2​=4.\n",
    "    Equal prior probabilities for each class.\n",
    "\n",
    "We want to calculate:\n",
    "P(Class=A∣X1=3,X2=4)P(Class=A∣X1​=3,X2​=4)\n",
    "P(Class=B∣X1=3,X2=4)P(Class=B∣X1​=3,X2​=4)\n",
    "\n",
    "Using Bayes' theorem, the formula for the posterior probability of a class given the features is:\n",
    "P(Class∣X1,X2)=P(X1,X2∣Class)⋅P(Class)P(X1,X2)P(Class∣X1​,X2​)=P(X1​,X2​)P(X1​,X2​∣Class)⋅P(Class)​\n",
    "\n",
    "Since we assume equal prior probabilities, P(Class=A)=P(Class=B)=0.5P(Class=A)=P(Class=B)=0.5.\n",
    "\n",
    "Now, let's calculate the likelihoods for each class based on the provided frequency table:\n",
    "P(X1=3∣Class=A)=410P(X1​=3∣Class=A)=104​\n",
    "P(X2=4∣Class=A)=310P(X2​=4∣Class=A)=103​\n",
    "\n",
    "P(X1=3∣Class=B)=17P(X1​=3∣Class=B)=71​\n",
    "P(X2=4∣Class=B)=37P(X2​=4∣Class=B)=73​\n",
    "\n",
    "Now, plug these values into Bayes' theorem:\n",
    "\n",
    "For Class A:\n",
    "P(Class=A∣X1=3,X2=4)=410⋅310⋅0.5P(X1=3,X2=4)\n",
    "P(Class=A∣X1​=3,X2​=4)=P(X1​=3,X2​=4)104​⋅103​⋅0.5​\n",
    "\n",
    "For Class B:\n",
    "P(Class=B∣X1=3,X2=4)=17⋅37⋅0.5P(X1=3,X2=4)P(Class=B∣X1​=3,X2​=4)=P(X1​=3,X2​=4)71​⋅73​⋅0.5​\n",
    "\n",
    "Now, compare the two probabilities and predict the class with the higher probability.\n",
    "\n",
    "This calculation requires finding the overall probability P(X1=3,X2=4)P(X1​=3,X2​=4), which can be computed as the sum of the individual probabilities for each class:\n",
    "\n",
    "P(X1=3,X2=4)=P(X1=3,X2=4∣Class=A)⋅P(Class=A)+P(X1=3,X2=4∣Class=B)⋅P(Class=B)P(X1​=3,X2​=4)=P(X1​=3,X2​=4∣Class=A)⋅P(Class=A)+P(X1​=3,X2​=4∣Class=B)⋅P(Class=B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
