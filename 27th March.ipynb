{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb11cb-4802-4aed-83ea-b085ec62ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. R-squared in Linear Regression:\n",
    "R-squared (coefficient of determination) is a statistical measure that represents the proportion of the variance in\n",
    "the dependent variable that is explained by the independent variables in a regression model. In the context of linear regression,\n",
    "it is often used to assess the goodness of fit of the model. R-squared values range from 0 to 1, with 1 indicating a perfect fit,\n",
    "meaning all variability in the dependent variable is explained by the independent variables.\n",
    "\n",
    "The formula for R-squared is:\n",
    "R2=1−Sum of Squared Residuals (SSR)Total Sum of Squares (SST)R2=1−Total Sum of Squares (SST)Sum of Squared Residuals (SSR)​\n",
    "\n",
    "Where:\n",
    "\n",
    "    SSRSSR is the sum of squared differences between the observed and predicted values.\n",
    "    SSTSST is the total sum of squares, representing the squared differences between the observed values and the mean of the \n",
    "    dependent variable.\n",
    "\n",
    "A higher R-squared value suggests a better fit of the model to the data.\n",
    "\n",
    "Q2. Adjusted R-squared:\n",
    "Adjusted R-squared is a modification of R-squared that adjusts for the number of predictors in the model. It penalizes the \n",
    "inclusion of irrelevant variables that do not significantly improve the model. The adjusted R-squared is calculated using the formula:\n",
    "Adjusted R2=1−(1−R2)(n−1)n−k−1Adjusted R2=1−n−k−1(1−R2)(n−1)​\n",
    "\n",
    "Where:\n",
    "\n",
    "    nn is the number of observations.\n",
    "    kk is the number of predictors.\n",
    "\n",
    "Q3. When to use Adjusted R-squared:\n",
    "Adjusted R-squared is more appropriate when comparing models with different numbers of predictors. It penalizes models that\n",
    "include unnecessary variables, making it useful for model selection. When the number of predictors is small, adjusted R-squared \n",
    "is typically close to R-squared.\n",
    "\n",
    "Q4. RMSE, MSE, and MAE:\n",
    "\n",
    "    RMSE (Root Mean Squared Error): It is the square root of the average of squared differences between predicted and observed values.\n",
    "    RMSE=∑i=1n(yi−y^i)2nRMSE=n∑i=1n​(yi​−y^​i​)2​\n",
    "\n",
    "    ​\n",
    "\n",
    "    MSE (Mean Squared Error): It is the average of squared differences between predicted and observed values.\n",
    "    MSE=∑i=1n(yi−y^i)2nMSE=n∑i=1n​(yi​−y^​i​)2​\n",
    "\n",
    "    MAE (Mean Absolute Error): It is the average of absolute differences between predicted and observed values.\n",
    "    MAE=∑i=1n∣yi−y^i∣nMAE=n∑i=1n​∣yi​−y^​i​∣​\n",
    "\n",
    "Q5. Advantages and Disadvantages of Metrics:\n",
    "\n",
    "    RMSE: Sensitive to outliers, provides a balanced view of errors.\n",
    "    MSE: Squaring emphasizes larger errors, sensitive to outliers.\n",
    "    MAE: Less sensitive to outliers, provides a robust measure.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "    Provide quantitative measures of model performance.\n",
    "    Easy to interpret.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "    Sensitive to outliers.\n",
    "    Units of measurement matter.\n",
    "\n",
    "Q6. Lasso Regularization:\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) is a regularization technique in linear regression that adds a \n",
    "penalty term to the cost function based on the absolute values of the coefficients. It encourages sparsity by driving some \n",
    "coefficients to exactly zero. This helps in feature selection.\n",
    "\n",
    "Differences from Ridge:\n",
    "\n",
    "    Lasso can lead to exactly zero coefficients, effectively performing feature selection, while Ridge can only shrink \n",
    "    coefficients close to zero.\n",
    "    Lasso tends to select one feature from a group of correlated features, while Ridge may shrink all of them equally.\n",
    "\n",
    "Q7. Regularized Models for Overfitting:\n",
    "Regularized linear models, such as Lasso and Ridge, help prevent overfitting by adding penalty terms to the model's cost function. \n",
    "This prevents the model from fitting the training data too closely, making it more generalizable to new, unseen data.\n",
    "\n",
    "Example: In Lasso regularization, the penalty term encourages the model to choose a subset of features by driving some\n",
    "coefficients to zero. This helps in avoiding overfitting by eliminating unnecessary predictors.\n",
    "\n",
    "Q8. Limitations of Regularized Linear Models:\n",
    "\n",
    "    Loss of Interpretability: Regularization may make interpretation of coefficients more challenging.\n",
    "    Selection of Regularization Parameter: The choice of the regularization parameter is crucial and may require tuning.\n",
    "    Assumption of Linearity: Regularized linear models assume a linear relationship between predictors and the response variable,\n",
    "    which may not always hold.\n",
    "\n",
    "Q9. Choosing Between RMSE and MAE:\n",
    "\n",
    "    Model A (RMSE of 10): Emphasizes larger errors, sensitive to outliers.\n",
    "    Model B (MAE of 8): Less sensitive to outliers, provides a robust measure.\n",
    "\n",
    "Choice depends on the specific goals and characteristics of the data. If outliers are critical and need to be handled carefully,\n",
    "Model B might be preferred.\n",
    "\n",
    "Limitations: Context matters, and the choice might not be universal for all situations.\n",
    "\n",
    "Q10. Choosing Between Ridge and Lasso:\n",
    "\n",
    "    Model A (Ridge with α=0.1α=0.1): Shrinkage without feature selection.\n",
    "    Model B (Lasso with α=0.5α=0.5): Feature selection, potentially driving some coefficients to zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
