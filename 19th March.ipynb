{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a20a5-6d28-4a29-ad5c-60d1e9983965",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Min-Max Scaling:\n",
    "\n",
    "Definition:\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale numerical features within a specific range,\n",
    "usually [0, 1]. The transformation is applied to each feature individually and is defined by the formula:\n",
    "\n",
    "Xscaled=X−min(X)max(X)−min(X)Xscaled​=max(X)−min(X)X−min(X)​\n",
    "\n",
    "Application:\n",
    "Let's say we have a dataset with a feature \"age\" ranging from 20 to 60. Applying Min-Max scaling would transform these values to a\n",
    "range of [0, 1].\n",
    "\n",
    "Xscaled=X−2060−20Xscaled​=60−20X−20​\n",
    "\n",
    "If X=30X=30, then the scaled value would be Xscaled=30−2060−20=14Xscaled​=60−2030−20​=41​.\n",
    "\n",
    "Q2. Unit Vector Scaling:\n",
    "\n",
    "Definition:\n",
    "The Unit Vector technique, also known as vector normalization or scaling to unit length, scales each data point to have a\n",
    "length of 1 while preserving its direction. It is often used in machine learning when the direction of the data points is\n",
    "more important than their absolute values.\n",
    "\n",
    "Xunit=X∥X∥Xunit​=∥X∥X​\n",
    "\n",
    "Difference from Min-Max Scaling:\n",
    "While Min-Max scaling squeezes the data into a specific range, Unit Vector scaling maintains the direction of the data \n",
    "points, converting them into vectors of unit length.\n",
    "\n",
    "Application Example:\n",
    "Let's say we have a dataset with two features, \"height\" and \"weight.\" The vector [height,weight][height,weight] \n",
    "is scaled to have a unit length, preserving the direction in a multi-dimensional space.\n",
    "\n",
    "Xunit=[height,weight]height2+weight2Xunit​=height2+weight2\n",
    "\n",
    "​[height,weight]​\n",
    "\n",
    "Q3. PCA (Principal Component Analysis):\n",
    "\n",
    "Definition:\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space,\n",
    "capturing the most significant variability in the data. It identifies the principal components, which are linear combinations of the original \n",
    "features.\n",
    "\n",
    "Application:\n",
    "Consider a dataset with multiple correlated features related to the physical dimensions of an object (length, width, height).\n",
    "PCA would identify the principal components that explain the maximum variance in the dataset, allowing us to represent the data\n",
    "in a lower-dimensional space.\n",
    "\n",
    "Q4. PCA and Feature Extraction:\n",
    "\n",
    "Relationship:\n",
    "PCA can be used for feature extraction by identifying the principal components that contribute the most to the variability in the data.\n",
    "These principal components serve as new features, allowing for dimensionality reduction while retaining the essential information.\n",
    "\n",
    "Example:\n",
    "In a dataset containing various economic indicators like GDP growth, inflation rate, and unemployment rate, \n",
    "PCA can be applied to extract a reduced set of features (principal components) that represent the most significant economic trends.\n",
    "\n",
    "Q5. Using Min-Max Scaling for a Food Delivery Recommendation System:\n",
    "\n",
    "Min-Max scaling can be applied to features such as \"price,\" \"rating,\" and \"delivery time\" in the food delivery dataset. \n",
    "For each feature, the transformation would be:\n",
    "\n",
    "Xscaled=X−min(X)max(X)−min(X)Xscaled​=max(X)−min(X)X−min(X)​\n",
    "\n",
    "This ensures that all features are scaled to a common range, making them comparable and preventing any \n",
    "particular feature from dominating others in the recommendation system.\n",
    "\n",
    "Q6. Using PCA for Dimensionality Reduction in Stock Price Prediction:\n",
    "\n",
    "In a stock price prediction project with numerous features related to financial data and market trends, \n",
    "PCA can be employed to reduce dimensionality. By identifying principal components, the dataset can be represented in\n",
    "a lower-dimensional space, capturing the most relevant information for predicting stock prices while reducing computational complexity.\n",
    "\n",
    "Q7. Min-Max Scaling for a Dataset [1,5,10,15,20][1,5,10,15,20]:\n",
    "\n",
    "To transform the values to a range of -1 to 1, the Min-Max scaling formula is applied:\n",
    "\n",
    "Xscaled=X−min(X)max(X)−min(X)Xscaled​=max(X)−min(X)X−min(X)​\n",
    "\n",
    "For the given dataset:\n",
    "Xscaled=X−120−1Xscaled​=20−1X−1​\n",
    "\n",
    "Q8. Feature Extraction using PCA for a Dataset [height,weight,age,gender,bloodpressure][height,weight,age,gender,bloodpressure]:\n",
    "\n",
    "PCA would identify the principal components based on the variability in the data. \n",
    "The number of principal components to retain depends on the desired level of explained variance. \n",
    "One might choose a number of components that, together, explain a sufficiently high percentage of the total variance, such as 95%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
